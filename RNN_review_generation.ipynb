{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"RNN_Final.ipynb","provenance":[{"file_id":"1zFBuC_NDBlEX65XLWWPcyHzwkOVB3_7n","timestamp":1622166773910}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU","language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"1fuqbm57Nh0H"},"source":["# pip install keras\n","# pip install tensorflow"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"057rm8g7Nhz-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622218927828,"user_tz":420,"elapsed":4421,"user":{"displayName":"Abby Chen","photoUrl":"https://lh5.googleusercontent.com/-jNsmaeVM3ZQ/AAAAAAAAAAI/AAAAAAAAChE/kkfo9qVLjQI/s64/photo.jpg","userId":"12134533523180251098"}},"outputId":"7f647953-b6f0-45c2-854f-ec069e2ef8bd"},"source":["import pandas as pd\n","import re\n","import random\n","import sys\n","import numpy as np\n","import pandas as pd\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","from keras.utils import np_utils\n","from keras import optimizers\n","import keras\n","from keras import layers\n","from nltk.tokenize import sent_tokenize,word_tokenize\n","import time"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Wup15B7Np4w","executionInfo":{"status":"ok","timestamp":1622218953005,"user_tz":420,"elapsed":23414,"user":{"displayName":"Abby Chen","photoUrl":"https://lh5.googleusercontent.com/-jNsmaeVM3ZQ/AAAAAAAAAAI/AAAAAAAAChE/kkfo9qVLjQI/s64/photo.jpg","userId":"12134533523180251098"}},"outputId":"021b16c8-172c-492b-d372-391efcfb89df"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e0qyNx2fKaO-"},"source":["# Load Preprocessed Data"]},{"cell_type":"code","metadata":{"id":"OuygamL0akuv"},"source":["review = {}\n","scores=pd.read_csv(\"/content/drive/My Drive/NLP/review_with_scores.csv\")\n","scores=scores.sort_values(\"automated_readability_index\",ascending=False)\n","review['preprocessed']=scores[\"text\"]\n","review['preprocessed'].reset_index(inplace=True,drop=True)\n","review['preprocessed']=review['preprocessed'].apply(lambda x: \"<SOR>\"+x+\"<EOR>\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TflbU9fSKox6"},"source":["#pick the top N high readibility to train\n","sample_size=8000\n","review['sample'] = review['preprocessed4'][0:sample_size] "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"89EYDjByK9lS"},"source":["# Create character-level Corpus"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bg9UoPRaNh0I","executionInfo":{"status":"ok","timestamp":1622218977685,"user_tz":420,"elapsed":237,"user":{"displayName":"Abby Chen","photoUrl":"https://lh5.googleusercontent.com/-jNsmaeVM3ZQ/AAAAAAAAAAI/AAAAAAAAChE/kkfo9qVLjQI/s64/photo.jpg","userId":"12134533523180251098"}},"outputId":"848cbb23-241a-4031-b281-731dcf1d7336"},"source":["filename = 'short_reviews.txt'\n","review['sample'].to_csv(filename, header=None, index=None, sep=' ')\n","train_text = open('short_reviews.txt').read()\n","print('Corpus length:', len(train_text))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Corpus length: 2163801\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q7H4TwD_Nh0J","executionInfo":{"status":"ok","timestamp":1622218978984,"user_tz":420,"elapsed":154,"user":{"displayName":"Abby Chen","photoUrl":"https://lh5.googleusercontent.com/-jNsmaeVM3ZQ/AAAAAAAAAAI/AAAAAAAAChE/kkfo9qVLjQI/s64/photo.jpg","userId":"12134533523180251098"}},"outputId":"9208c759-8023-4628-97be-def1e3f30406"},"source":["# set default value for key parameters\n","maxlen=60\n","step=1\n","chars = ['\\n',' ','!', '$', \"'\",  ',', '.', '?', '@', '\"',\n","             '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \n","             'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', \n","             'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', \n","             'U', 'V', 'W', 'X', 'Y', 'Z', \n","             'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j',\n","             'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', \n","             'u', 'v', 'w', 'x', 'y', 'z','<','>']\n","             \n","print('Unique characters:', len(chars))\n","\n","#create dictionary\n","char_indices = {}\n","for char in chars:\n","  char_indices[char]=chars.index(char)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Unique characters: 74\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mZCAafU0Mfx0"},"source":["# Train Model"]},{"cell_type":"code","metadata":{"id":"XiOC-Eh7O-yW"},"source":["from keras import models\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OxzE6cEVO-jj"},"source":["## Load the model\n","8000_Best_Model_RNN.hdf5 saved from the model below and it took 7hrs to run"]},{"cell_type":"code","metadata":{"id":"FBZyHNmghAKK"},"source":["model = tf.keras.models.load_model(\"/content/drive/My Drive/NLP/8000_Best_Model_RNN.hdf5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xqegfHliPPpE"},"source":["## RNN Model"]},{"cell_type":"code","metadata":{"id":"zwqH3rnFNh0L"},"source":["#Model\n","model = keras.models.Sequential()\n","model.add(layers.LSTM(1024, input_shape=(maxlen, len(chars)),return_sequences=True))\n","model.add(layers.LSTM(1024, input_shape=(maxlen, len(chars))))\n","model.add(layers.Dense(len(chars), activation='softmax'))\n","optimizer = keras.optimizers.Adam(learning_rate=0.001) # default setting: learning_rate=0.001\n","model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n","filepath=\"/content/drive/My Drive/NLP/8000_Best_Model_RNN.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, \n","                             monitor='loss', \n","                             verbose=1, \n","                             save_best_only=True, \n","                             mode='min')\n","reduce_lr = ReduceLROnPlateau(monitor='loss', \n","                              factor=0.5,\n","                              patience=1, \n","                              min_lr=0.00001) # default setting: min_lr=0.00001\n","callbacks_list = [checkpoint, reduce_lr]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hXI3pSKqNh0K"},"source":["#necessary to process large data\n","#X : tensor 把text split to len(textchunk)-60 files --> 60個character為單位去拆分\n","#txtChunk: text file\n","def getChunk(txtChunk, maxlen=60, step=1):\n","    sentences = []\n","    next_chars = []\n","    for i in range(0, len(txtChunk) - maxlen, step):\n","        sentences.append(txtChunk[i : i + maxlen])\n","        next_chars.append(txtChunk[i + maxlen])\n","    print('number of sequences:', len(sentences))\n","    print('Vectorization...')\n","    X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n","    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n","    for i, sentence in enumerate(sentences):\n","        for t, char in enumerate(sentence):\n","            X[i, t, char_indices[char]] = 1\n","            y[i, char_indices[next_chars[i]]] = 1\n","    return [X, y]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"74R2aFwcanb8","executionInfo":{"status":"ok","timestamp":1622226699252,"user_tz":420,"elapsed":7182396,"user":{"displayName":"Abby Chen","photoUrl":"https://lh5.googleusercontent.com/-jNsmaeVM3ZQ/AAAAAAAAAAI/AAAAAAAAChE/kkfo9qVLjQI/s64/photo.jpg","userId":"12134533523180251098"}},"outputId":"4d5feaa8-6a8c-4c75-ac03-d6d61edb7f39"},"source":["t0 = time.time()\n","for iteration in range(1, 20):\n","    print('Iteration', iteration)\n","    with open(\"short_reviews.txt\") as f:\n","        for chunk in iter(lambda: f.read(90000), \"\"): #split entire text to many chunks and each one has 90000 chars \n","            X, y = getChunk(chunk)\n","            model.fit(X, y, batch_size=128, epochs=1, callbacks=callbacks_list)\n"," t1 = time.time()           \n"," print(\"Training took {:.2f}s\".format(t1 - t0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration 1\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5961\n","\n","Epoch 00001: loss improved from inf to 0.59610, saving model to /content/drive/My Drive/NLP/8000_keep_Best_Model_RNN.hdf5\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5630\n","\n","Epoch 00001: loss improved from 0.59610 to 0.56298, saving model to /content/drive/My Drive/NLP/8000_keep_Best_Model_RNN.hdf5\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5460\n","\n","Epoch 00001: loss improved from 0.56298 to 0.54603, saving model to /content/drive/My Drive/NLP/8000_keep_Best_Model_RNN.hdf5\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5416\n","\n","Epoch 00001: loss improved from 0.54603 to 0.54162, saving model to /content/drive/My Drive/NLP/8000_keep_Best_Model_RNN.hdf5\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5545\n","\n","Epoch 00001: loss did not improve from 0.54162\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5447\n","\n","Epoch 00001: loss did not improve from 0.54162\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5436\n","\n","Epoch 00001: loss did not improve from 0.54162\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5482\n","\n","Epoch 00001: loss did not improve from 0.54162\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5402\n","\n","Epoch 00001: loss improved from 0.54162 to 0.54020, saving model to /content/drive/My Drive/NLP/8000_keep_Best_Model_RNN.hdf5\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5387\n","\n","Epoch 00001: loss improved from 0.54020 to 0.53867, saving model to /content/drive/My Drive/NLP/8000_keep_Best_Model_RNN.hdf5\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5428\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.6700\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.6663\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.6578\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.6541\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.6516\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.6389\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 72s 102ms/step - loss: 0.6430\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.6498\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 72s 102ms/step - loss: 0.6514\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.6348\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.6404\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.6368\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.6418\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 3741\n","Vectorization...\n","30/30 [==============================] - 3s 100ms/step - loss: 0.6846\n","\n","Epoch 00001: loss did not improve from 0.53867\n","Iteration 2\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.6121\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5862\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 72s 102ms/step - loss: 0.5741\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5720\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5834\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5696\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5637\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5765\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5646\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5629\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5626\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.6191\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.6212\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.6163\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.6076\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.6055\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5976\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5994\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.6095\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.6068\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5972\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5966\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5959\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.6042\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 3741\n","Vectorization...\n","30/30 [==============================] - 3s 100ms/step - loss: 0.6193\n","\n","Epoch 00001: loss did not improve from 0.53867\n","Iteration 3\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5932\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5735\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5626\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5617\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5678\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5587\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5560\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5653\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5574\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5532\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5515\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5899\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5896\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5838\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5815\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5777\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5708\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5698\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5785\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5789\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5686\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5717\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5666\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5740\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 3741\n","Vectorization...\n","30/30 [==============================] - 3s 99ms/step - loss: 0.5866\n","\n","Epoch 00001: loss did not improve from 0.53867\n","Iteration 4\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5722\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5557\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5423\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5461\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5573\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5468\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5401\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5528\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5403\n","\n","Epoch 00001: loss did not improve from 0.53867\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5373\n","\n","Epoch 00001: loss improved from 0.53867 to 0.53729, saving model to /content/drive/My Drive/NLP/8000_keep_Best_Model_RNN.hdf5\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5318\n","\n","Epoch 00001: loss improved from 0.53729 to 0.53182, saving model to /content/drive/My Drive/NLP/8000_keep_Best_Model_RNN.hdf5\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5670\n","\n","Epoch 00001: loss did not improve from 0.53182\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5683\n","\n","Epoch 00001: loss did not improve from 0.53182\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5597\n","\n","Epoch 00001: loss did not improve from 0.53182\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5579\n","\n","Epoch 00001: loss did not improve from 0.53182\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5549\n","\n","Epoch 00001: loss did not improve from 0.53182\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5463\n","\n","Epoch 00001: loss did not improve from 0.53182\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5503\n","\n","Epoch 00001: loss did not improve from 0.53182\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5551\n","\n","Epoch 00001: loss did not improve from 0.53182\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5565\n","\n","Epoch 00001: loss did not improve from 0.53182\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5450\n","\n","Epoch 00001: loss did not improve from 0.53182\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 101ms/step - loss: 0.5498\n","\n","Epoch 00001: loss did not improve from 0.53182\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5420\n","\n","Epoch 00001: loss did not improve from 0.53182\n","number of sequences: 89940\n","Vectorization...\n","703/703 [==============================] - 71s 102ms/step - loss: 0.5484\n","\n","Epoch 00001: loss did not improve from 0.53182\n","number of sequences: 3741\n","Vectorization...\n","30/30 [==============================] - 3s 100ms/step - loss: 0.5689\n","\n","Epoch 00001: loss did not improve from 0.53182\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tltN1zifJtWD"},"source":["# Review Generation"]},{"cell_type":"code","metadata":{"id":"7LCk_kuihIF6"},"source":["def generate_reviews_v2(temperature = 0.75, maxlen = 60, len_review=60):\n","    seed_text = ''\n","    while len(seed_text) < 80:\n","      seed_text += review['trained'][random.randint(0, len(review['trained']))] \n","    start_index = np.random.randint(0, len(seed_text) - maxlen - 1)\n","    generated_text = seed_text[start_index: start_index + maxlen]\n","    output = seed_text[: start_index + maxlen]\n","    word = ''\n","\n","    for i in range(len_review):\n","      \n","      sampled = np.zeros((1, maxlen, len(chars)))\n","      for t, char in enumerate(generated_text):\n","          sampled[0, t, char_indices[char]] = 1.\n","      \n","      preds = model.predict(sampled, verbose=0)[0]\n","      preds = np.asarray(preds).astype('float64')\n","      preds = np.log(preds) / temperature\n","      exp_preds = np.exp(preds)\n","      preds = exp_preds / np.sum(exp_preds)\n","      probas = np.random.multinomial(1, preds, 1)\n","      next_index = np.argmax(probas)\n","      next_char = chars[next_index]\n","      output += next_char\n","      generated_text = output[-60:]\n","    \n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"VhwEYCypfzz6","executionInfo":{"status":"ok","timestamp":1622219081696,"user_tz":420,"elapsed":7073,"user":{"displayName":"Abby Chen","photoUrl":"https://lh5.googleusercontent.com/-jNsmaeVM3ZQ/AAAAAAAAAAI/AAAAAAAAChE/kkfo9qVLjQI/s64/photo.jpg","userId":"12134533523180251098"}},"outputId":"c3d55111-aed5-45e0-c678-aca8ee0a27c5"},"source":["output=generate_reviews_v2(temperature = 0.75, maxlen = 60, len_review=100)\n","output"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<SOR>We had a company dinner at rosewood and it could not have been a better experience . Location from our hotel downtown was easily accessible on scooters as well as uber , the activities available on the grass as our large group arrived was a huge plus and the staff was kind and accommodating to each and charge us promptly and helped me kind of food to put it until I actuall'"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86},"id":"vE_u_MbgzFfX","executionInfo":{"status":"ok","timestamp":1622219190675,"user_tz":420,"elapsed":2483,"user":{"displayName":"Abby Chen","photoUrl":"https://lh5.googleusercontent.com/-jNsmaeVM3ZQ/AAAAAAAAAAI/AAAAAAAAChE/kkfo9qVLjQI/s64/photo.jpg","userId":"12134533523180251098"}},"outputId":"adf77898-4ac3-49da-c1a3-ae0cc51b4ba8"},"source":["output=generate_reviews_v2(temperature = 0.75, maxlen = 60, len_review=60)\n","output"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<SOR>This place has incredible food and I am pretty sure that many of the negative reviews here are at least a little bit racist . The soup dumplings are amazing , and I would recommend the eggplant and the chinese broccoli or sauted greens to accompany whatever heavier dishes have the thing they have tastes like the eggplant , bl'"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"82W77s7zzclF","executionInfo":{"status":"ok","timestamp":1622219284518,"user_tz":420,"elapsed":1263,"user":{"displayName":"Abby Chen","photoUrl":"https://lh5.googleusercontent.com/-jNsmaeVM3ZQ/AAAAAAAAAAI/AAAAAAAAChE/kkfo9qVLjQI/s64/photo.jpg","userId":"12134533523180251098"}},"outputId":"51110974-c8d2-4c96-efa9-a25953975b1e"},"source":["output=generate_reviews_v2(temperature = 0.75, maxlen = 60, len_review=30)\n","output"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<SOR>Absolutely amazing french food , never disappointing . We started with escargot and oysters beautiful way to get the appetite going , with some fresh bread and a cocktail . Our server was friendly , courteous and knowledgeable but not pompone establishments when they ra'"]},"metadata":{"tags":[]},"execution_count":24}]}]}