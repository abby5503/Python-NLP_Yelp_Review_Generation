{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data_Preprocessing.ipynb","provenance":[{"file_id":"1VSHuorK7axKEHUOXiTrkSnLJVblXJFNx","timestamp":1622409472663},{"file_id":"1-cOAeKc8bK5phPl1LHK92GAcL_yjbsLJ","timestamp":1622146707820}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"qsIkOhY5K3Af","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622403149034,"user_tz":420,"elapsed":2817,"user":{"displayName":"Abby Chen","photoUrl":"https://lh5.googleusercontent.com/-jNsmaeVM3ZQ/AAAAAAAAAAI/AAAAAAAAChE/kkfo9qVLjQI/s64/photo.jpg","userId":"12134533523180251098"}},"outputId":"f6b0d9f0-07ce-4708-d8f6-dfc004d1350c"},"source":["import os\n","import time\n","import datetime\n","from google.colab import drive\n","import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt\n","% matplotlib inline\n","import nltk\n","nltk.download('punkt')\n","from nltk.corpus import wordnet\n","import re\n","import spacy\n","from spacy_readability import Readability\n","# pip install spacy-readability"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qDTzNIktK_F_","executionInfo":{"status":"ok","timestamp":1622402093732,"user_tz":420,"elapsed":35279,"user":{"displayName":"Abby Chen","photoUrl":"https://lh5.googleusercontent.com/-jNsmaeVM3ZQ/AAAAAAAAAAI/AAAAAAAAChE/kkfo9qVLjQI/s64/photo.jpg","userId":"12134533523180251098"}},"outputId":"79ea01c9-aba7-4676-a7b9-e4dfcbb6bb77"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PFYJ8eg3H0ij"},"source":["# Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"qH0PuCfcH6BZ"},"source":["review = {}\n","review['raw'] = pd.read_csv(\"/content/drive/My Drive/NLP/yelp_restaurant_only_review.csv\")\n","review['raw'] = review['raw'].sort_values(\"text\")\n","review['filtered'] = review['raw'][review['raw'].stars==5.0][\"text\"]\n","review['filtered'] = review['filtered'].drop_duplicates()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UTINTdNDH6vL"},"source":["review['preprocessed'] = review['filtered'].to_frame()\n","review['preprocessed'][\"text\"] = review['preprocessed'][\"text\"].apply(lambda x: x.replace(\"\\n\",\" \"))\n","review['preprocessed'][\"text\"] = review['preprocessed'][\"text\"].apply(lambda x: x.replace(u'\\xa0', u' '))\n","review['preprocessed'][\"text\"] = review['preprocessed'][\"text\"].apply(lambda x: x.replace(u'\\u2006', u' '))\n","review['preprocessed'][\"text\"] = review['preprocessed'][\"text\"].apply(lambda x: x.replace(u'\\u2009', u' '))\n","review['preprocessed'][\"text\"] = review['preprocessed'][\"text\"].apply(lambda x: x.replace(u'\\u3000', u' '))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fqZDeamlH9_B"},"source":["# common preprocess\n","\n","def sentence_preprocess(x):\n","\n","    #keep english review\n","    pattern=re.compile(r'[A-Za-z0-9]*\\s+[<=>#$%&?.!,\"{}()]*[A-Za-z0-9]*[<=>#$%&?.!,\"{}()]*')\n","    text_1=pattern.findall(x)\n","    step_1=''.join(map(str, text_1)).strip()\n","    \n","    #remove duplicate punctuation  \n","    newtext = []\n","    for k, g in groupby(step_1):\n","        if k in punctuation:\n","            newtext.append(k)\n","        else:\n","            newtext.extend(g)\n","    step_2=''.join(newtext)\n","\n","    #lower\n","    lower=step_2.lower()\n","\n","    #capitalize by sentence\n","    sent=sent_tokenize(lower)\n","    capitalized_sent=[]\n","\n","    for i in sent:\n","      capitalized_sent.append(i.capitalize())\n","    step_3=' '.join(map(str, capitalized_sent)).strip()\n","    \n","    # replace single i to I\n","    pattern2=re.compile(r'[.?!_$@\\'\\s][i][.?!_$@\\'\\s]')\n","    if pattern2.findall(step_3) != None:      \n","      step_4 = re.sub(pattern2, ' I ', step_3)\n","    \n","    #remove unwanted punctuations\n","    remove_punc=set(punctuation)-set([\".\",\",\",\"?\",\"'\",\"!\",\"_\",\"$\",\"@\"])\n","    for i in step_4:\n","      if i in remove_punc:\n","        step_4= step_4.replace(i,\" \")\n","\n","    #remove duplicate space\n","    step_5=re.sub(' +', ' ',step_4 )\n","    return step_5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nkMB4lKjIHsb"},"source":["review['preprocessed1'] = review['preprocessed'][\"text\"].apply(lambda x: sentence_preprocess(x))\n","review['preprocessed1'] = review['preprocessed1'].loc[(review['preprocessed1'].str.len() < 600) & (review['preprocessed1'].str.len() > 60)]\n","review['preprocessed1'].reset_index(inplace=True,drop=True)\n","review['preprocessed1']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vaUykN_bINez"},"source":["# recovery common abbreviation\n","def abbreviation_return(word):\n","  replacement_patterns = [\n","    (r'^won\\'t$', 'will not'),\n","    (r'^wont$', 'will not'),\n","    (r'^can\\'t$', 'cannot'),\n","    (r'^cant$', 'cannot'),\n","    (r'^didnt$', 'did not'),\n","    (r'^dont$', 'do not'),\n","    (r'^doesnt$', 'does not'),\n","    (r'^i\\'m$', 'I am'),\n","    (r'^im$', 'I am'),\n","    (r'^Im$', 'I am'),\n","    (r'^ive$', 'I have'),\n","    (r'^Ive$', 'I have'),\n","    (r'^ain\\'t$', 'is not'),\n","    (r'^aint$', 'is not'),\n","    (r'(\\w+)\\'ll', '\\g<1> will'),\n","    (r'(\\w+)n\\'t', '\\g<1> not'),\n","    (r'(\\w+)\\'ve', '\\g<1> have'),\n","    (r'(\\w+)\\'s', '\\g<1> is'),\n","    (r'(\\w+)\\'re', '\\g<1> are'),\n","    (r'(\\w+)\\'d', '\\g<1> would')]\n","  patterns = [(re.compile(regex), repl) for (regex, repl) in replacement_patterns]\n","  for (pattern, repl) in patterns:\n","      if pattern.search(word) != None:\n","        return re.subn(pattern, repl, word)[0]\n","  return word\n","\n","def recovery(x):\n","  words= nltk.word_tokenize(x)\n","  return ' '.join(map(str, [abbreviation_return(item) for item in words])).strip()\n","\n","review['preprocessed2']=review['preprocessed1'].apply(lambda x: recovery(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"csYTjBnpITyt"},"source":["# remove the word like \"sooooooo\" or \"gooooooood\" \n","def Remove_repeat_word(sentence):\n","  words= nltk.word_tokenize(sentence)\n","  pattern = re.compile(r'^(\\w*)(\\w)\\2(\\w*)$')\n","  repl = r'\\1\\2\\3'\n","  for i in range(len(words)):\n","    while (pattern.findall(words[i]) != []) & (wordnet.synsets(words[i]) == []):\n","      words[i]=pattern.sub(repl, words[i])\n","  return ' '.join(map(str, words))\n","\n","review['preprocessed3']=review['preprocessed2'].apply(lambda x: Remove_repeat_word(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYOZM1MlIYdz"},"source":["\n","nlp = spacy.load('en')\n","read = Readability()\n","nlp.add_pipe(read, last=True)\n","\n","scores = []\n","for i in review['preprocessed3']:\n","  doc = nlp(i)\n","  dict={\"text\":i,\\\n","        \"automated_readability_index\":doc._.automated_readability_index}\n","  scores.append(dict)\n","scores = pd.DataFrame(scores)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ft0ANyY_JnVC"},"source":["scores.to_csv(\"/content/drive/My Drive/NLP/review_with_scores.csv\")"],"execution_count":null,"outputs":[]}]}